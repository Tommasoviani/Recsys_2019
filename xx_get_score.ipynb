{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get result score notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Given a result.csv dataset we have the MRR associated to the given algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandasql as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv = pd.read_csv(\"~/Documents/Tesi/trivagoRecSysChallengeData2019_v2/prova1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_csv = pd.read_csv(\"~/Documents/Tesi/trivagoRecSysChallengeData2019_v2/validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>session_id</th>\n      <th>timestamp</th>\n      <th>step</th>\n      <th>item_recommendations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0004Q49X39PY</td>\n      <td>9de47d9a66494</td>\n      <td>1541641157</td>\n      <td>1</td>\n      <td>2213014 3184842 2292254 3812004 8153310 320289...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0004Q49X39PY</td>\n      <td>beea5c27030cb</td>\n      <td>1541561202</td>\n      <td>1</td>\n      <td>3812004 3505150 2292254 3202894 4476010 710135...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00071784XQ6B</td>\n      <td>9617600e1ba7c</td>\n      <td>1541630328</td>\n      <td>2</td>\n      <td>22721 22854 16121 22819 3067559 22764 1478189 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0008BO33KUQ0</td>\n      <td>2d0e2102ee0dc</td>\n      <td>1541636411</td>\n      <td>6</td>\n      <td>507861 2176280 1669587 4272108 3133204 502066 ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000GO9NY6P4M</td>\n      <td>55dbafdbb9bab</td>\n      <td>1541594662</td>\n      <td>2</td>\n      <td>157710 1287958 160577 319866 483691 1618677 40...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "        user_id     session_id   timestamp  step  \\\n0  0004Q49X39PY  9de47d9a66494  1541641157     1   \n1  0004Q49X39PY  beea5c27030cb  1541561202     1   \n2  00071784XQ6B  9617600e1ba7c  1541630328     2   \n3  0008BO33KUQ0  2d0e2102ee0dc  1541636411     6   \n4  000GO9NY6P4M  55dbafdbb9bab  1541594662     2   \n\n                                item_recommendations  \n0  2213014 3184842 2292254 3812004 8153310 320289...  \n1  3812004 3505150 2292254 3202894 4476010 710135...  \n2  22721 22854 16121 22819 3067559 22764 1478189 ...  \n3  507861 2176280 1669587 4272108 3133204 502066 ...  \n4  157710 1287958 160577 319866 483691 1618677 40...  "
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_into_df(file):\n",
    "    \"\"\"Read csv file into data frame.\"\"\"\n",
    "    df = (\n",
    "        pd.read_csv(file)\n",
    "            .set_index(['user_id', 'session_id', 'timestamp', 'step'])\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_rranks_range(start, end):\n",
    "    \"\"\"Generate reciprocal ranks for a given list length.\"\"\"\n",
    "\n",
    "    return 1.0 / (np.arange(start, end) + 1)\n",
    "\n",
    "\n",
    "def convert_string_to_list(df, col, new_col):\n",
    "    \"\"\"Convert column from string to list format.\"\"\"\n",
    "    fxn = lambda arr_string: [int(item) for item in str(arr_string).split(\" \")]\n",
    "\n",
    "    mask = ~(df[col].isnull())\n",
    "\n",
    "    df[new_col] = df[col]\n",
    "    df.loc[mask, new_col] = df[mask][col].map(fxn)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_reciprocal_ranks(ps):\n",
    "    \"\"\"Calculate reciprocal ranks for recommendations.\"\"\"\n",
    "    mask = ps.reference == np.array(ps.item_recommendations)\n",
    "\n",
    "    if mask.sum() == 1:\n",
    "        rranks = generate_rranks_range(0, len(ps.item_recommendations))\n",
    "        return np.array(rranks)[mask].min()\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def score_submissions(subm_csv, gt_csv, objective_function):\n",
    "    \"\"\"Score submissions with given objective function.\"\"\"\n",
    "\n",
    "    print(f\"Reading ground truth data {gt_csv} ...\")\n",
    "    df_gt = read_into_df(gt_csv)\n",
    "\n",
    "    print(f\"Reading submission data {subm_csv} ...\")\n",
    "    df_subm = read_into_df(subm_csv)\n",
    "\n",
    "    # create dataframe containing the ground truth to target rows\n",
    "    cols = ['reference', 'impressions', 'prices']\n",
    "    df_key = df_gt.loc[:, cols]\n",
    "\n",
    "    # append key to submission file\n",
    "    df_subm_with_key = df_key.join(df_subm, how='inner')\n",
    "    df_subm_with_key.reference = df_subm_with_key.reference.astype(int)\n",
    "    df_subm_with_key = convert_string_to_list(\n",
    "        df_subm_with_key, 'item_recommendations', 'item_recommendations'\n",
    "    )\n",
    "\n",
    "    # score each row\n",
    "    df_subm_with_key['score'] = df_subm_with_key.apply(objective_function, axis=1)\n",
    "    mrr = df_subm_with_key.score.mean()\n",
    "\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_COLUMNS = {'user_id', 'session_id', 'timestamp',\n",
    "                      'step', 'item_recommendations'}\n",
    "\n",
    "\n",
    "def get_test_session_set(df_test):\n",
    "    \"\"\"Identify sessions for which predictions have to be made.\"\"\"\n",
    "\n",
    "    mask = df_test.reference.isnull() & (df_test.action_type == 'clickout item')\n",
    "    test_session_set = set(df_test[mask]['session_id'])\n",
    "\n",
    "    return test_session_set\n",
    "\n",
    "\n",
    "def check_passed(check=True):\n",
    "    \"\"\"Print result of check.\"\"\"\n",
    "\n",
    "    if check:\n",
    "        print('> check passed')\n",
    "    else:\n",
    "        print('x check failed')\n",
    "\n",
    "\n",
    "def check_duplicates(df):\n",
    "    \"\"\"Check if there are duplicate sessions in the df.\"\"\"\n",
    "\n",
    "    check_dupl = (len(df['session_id']) == len(df['session_id'].unique()))\n",
    "\n",
    "    return check_dupl\n",
    "\n",
    "\n",
    "def check_columns(df):\n",
    "    \"\"\"Check if the submission has the correct column names.\"\"\"\n",
    "\n",
    "    check_cols = (set(df.columns) == SUBMISSION_COLUMNS)\n",
    "\n",
    "    return check_cols\n",
    "\n",
    "\n",
    "def check_sessions(df_subm, df_test):\n",
    "    \"\"\"Check if the submission contains the correct sessions.\"\"\"\n",
    "\n",
    "    set_test_sessions = get_test_session_set(df_test)\n",
    "    set_subm_sessions = set(df_subm['session_id'])\n",
    "    check_sess = (set_test_sessions == set_subm_sessions)\n",
    "\n",
    "    return check_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission_target(df):\n",
    "    \"\"\"Identify target rows with missing click outs.\"\"\"\n",
    "\n",
    "    mask = df[\"reference\"].isnull() & (df[\"action_type\"] == \"clickout item\")\n",
    "    df_out = df[mask]\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> check passed\n"
     ]
    }
   ],
   "source": [
    "check_cols = check_columns(result_csv)\n",
    "check_passed(check_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> check passed\n"
     ]
    }
   ],
   "source": [
    "check_dupl = check_duplicates(result_csv)\n",
    "check_passed(check_dupl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x check failed\n"
     ]
    }
   ],
   "source": [
    "check_sess = check_sessions(result_csv, gt_csv)\n",
    "check_passed(check_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ground truth data ~/Documents/Tesi/trivagoRecSysChallengeData2019_v2/validation.csv ...\n",
      "Reading submission data ~/Documents/Tesi/trivagoRecSysChallengeData2019_v2/prova1.csv ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5082995322768837"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr = score_submissions('~/Documents/Tesi/trivagoRecSysChallengeData2019_v2/prova1.csv', \n",
    "                        \"~/Documents/Tesi/trivagoRecSysChallengeData2019_v2/validation.csv\", get_reciprocal_ranks)\n",
    "mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ground truth data ~/Documents/Tesi/trivagoRecSysChallengeData2019_v2/validation.csv ...\n",
      "Reading submission data ~/Documents/Tesi/trivagoRecSysChallengeData2019_v2/formula_interaction03.csv ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5493158022778003"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr = score_submissions(\"~/Documents/Tesi/trivagoRecSysChallengeData2019_v2/formula_interaction03.csv\", \n",
    "                        \"~/Documents/Tesi/trivagoRecSysChallengeData2019_v2/validation.csv\", get_reciprocal_ranks)\n",
    "mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The `my_formula` algorithm gets 0.549 MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda9004609c197446b5b22406ff27f8a43b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}